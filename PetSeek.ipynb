{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fai-sDBGlFP3"
      },
      "outputs": [],
      "source": [
        "# Important Installations\n",
        "!pip install faiss-cpu gradio torchvision pillow\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation des Biblioth√©ques\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import faiss\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Connexion √† Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Sp√©cification du dossier d‚Äôimages\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/ImageSearch\"\n",
        "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "# Chargement des chemins des Images\n",
        "def load_images():\n",
        "    return [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR)\n",
        "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"))]\n",
        "\n",
        "# Chargement du mod√®le CLIP\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Construire la base de donn√©es des caract√©ristiques d'Images\n",
        "image_paths = load_images()\n",
        "\n",
        "# Extraction des caract√©ristiques (embeddings) des images avec CLIP\n",
        "def extract_image_features(image_path):\n",
        "    try:\n",
        "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feature = model.encode_image(image).cpu().numpy().flatten()\n",
        "        return feature / np.linalg.norm(feature)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Cr√©ation de la base de donn√©es FAISS\n",
        "image_features = np.array(\n",
        "    [feature for img_path in image_paths if (feature := extract_image_features(img_path)) is not None],\n",
        "    dtype=np.float32\n",
        ")\n",
        "\n",
        "if len(image_features) == 0:\n",
        "    raise ValueError(\"No valid image features extracted. Check your image directory and formats.\")\n",
        "\n",
        "dimension = image_features.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(dimension)  # Using inner product for cosine similarity\n",
        "faiss.normalize_L2(image_features)\n",
        "faiss_index.add(image_features)\n",
        "\n",
        "# Encodage des requ√™tes textuelles\n",
        "def extract_text_features(text):\n",
        "    text_tokens = clip.tokenize([text]).to(device)\n",
        "    with torch.no_grad():\n",
        "        feature = model.encode_text(text_tokens).cpu().numpy().flatten()\n",
        "    return feature / np.linalg.norm(feature)\n",
        "\n",
        "# Recherche des images les plus pertinentes\n",
        "def search_images(query_text, top_k=5, threshold=0.5):\n",
        "    text_feature = extract_text_features(query_text)\n",
        "    text_feature = np.expand_dims(text_feature, axis=0)\n",
        "    distances, indices = faiss_index.search(text_feature, top_k)\n",
        "    results = [(image_paths[i], 1 - distances[0][idx]) for idx, i in enumerate(indices[0])]\n",
        "\n",
        "    # Filtrer les r√©sultats pour n‚Äôafficher que ceux dont la similarit√© d√©passe le seuil\n",
        "    filtered_results = [(path, score) for path, score in results if score > threshold]\n",
        "\n",
        "    return [(Image.open(path), f\"Similarity: {score:.2f}\") for path, score in filtered_results]\n",
        "\n",
        "# Style CSS personnalis√© pour l'interface\n",
        "custom_css = \"\"\"\n",
        "\n",
        "/* Changer la couleur du fond de l'interface */\n",
        ".gradio-container {\n",
        "    background-color: #fefae0 !important;\n",
        "    font-family: 'Segoe UI', sans-serif !important;\n",
        "}\n",
        "\n",
        "/* Changer la couleur du fond de l'interface en blanc */\n",
        ".gallery-gallery {\n",
        "    background-color: white !important;\n",
        "    border-radius: 10px;\n",
        "    padding: 10px;\n",
        "    overflow: auto;\n",
        "}\n",
        "\n",
        "/* Assurer que chaque image a un fond blanc */\n",
        ".gradio-gallery img {\n",
        "    background-color: white !important;\n",
        "    border-radius: 8px;\n",
        "    padding: 5px;\n",
        "    object-fit: cover;  /* Ensure images fill the available space */\n",
        "}\n",
        "\n",
        "/* Titre et texte */\n",
        "h1, h2, h3 {\n",
        "    color: #2c3e50;\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        "/* Style des boutons */\n",
        "button {\n",
        "    background-color: #ffd6a5 !important;\n",
        "    color: black !important;\n",
        "    font-weight: bold;\n",
        "    border: none !important;\n",
        "    padding: 10px 20px;\n",
        "    border-radius: 8px;\n",
        "    cursor: pointer;\n",
        "    transition: background-color 0.3s ease;\n",
        "}\n",
        "\n",
        "button:hover {\n",
        "    background-color: #ff9f1c !important;\n",
        "}\n",
        "\n",
        "input[type=\"text\"] {\n",
        "    background-color: #fff5d7 !important;\n",
        "    border: 1px solid #e0c68c !important;\n",
        "    padding: 10px;\n",
        "    border-radius: 8px;\n",
        "}\n",
        "\n",
        "/* Pour le layout c√¥te √† c√¥te */\n",
        ".flex-row {\n",
        "    display: flex;\n",
        "    gap: 30px;\n",
        "    align-items: flex-start;\n",
        "    justify-content: center;\n",
        "}\n",
        "\n",
        ".flex-column {\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    gap: 15px;\n",
        "    min-width: 300px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Interface Gradio\n",
        "with gr.Blocks(css=custom_css) as iface:\n",
        "    gr.Markdown(\"# üêæPetSeek\")\n",
        "\n",
        "    # Disposition en ligne (Search bar + Gallery)\n",
        "    with gr.Row(elem_classes=\"flex-row\"):\n",
        "        with gr.Column(elem_classes=\"flex-column\"):\n",
        "            input_text = gr.Textbox(label=\"Search for an animal\", placeholder=\"Ex: dog , cat...\")\n",
        "            search_btn = gr.Button(\"Search\")\n",
        "        with gr.Column(elem_classes=\"flex-column\"):\n",
        "            gallery = gr.Gallery(label=\"Results of the Search\", elem_classes=\"gallery-container\")\n",
        "\n",
        "    # Action de recherche\n",
        "    search_btn.click(fn=search_images, inputs=input_text, outputs=gallery)\n",
        "\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "NCSsyhxmfd6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}